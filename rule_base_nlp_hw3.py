# -*- coding: utf-8 -*-
"""Rule Base  NLP HW3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Om1a9sGcoaPsVNwwQQTZ1Fn9Wgopv6eq

# READ (AND DO) THESE INSTRUCTIONS BEFORE RUNNING THE NOTEBOOK
Some of the code below use libraries with a different version than Google Colab. For example, gensim uses numpy (1.26.4) and scipy (1.13.1) and Google Colab includes the versions 2.0.2 and 1.14.1 respectively in the setup.

In order to setup properly these dependencies you should:


1.   Run the following cell as usually. It will take 3-4 minutes and should finish with a `"You can now load the package via spacy.load('en_core_web_lg')"` message.
2.   **Restart the session [Menu: Runtime -> Restart Session] to update ALL dependencies.**
3.   **Run again** this cell and everything will work.
"""

#Install the gensim library
!pip install numpy==1.26.4
!pip install scipy==1.13.1
!pip install gensim
!pip install smart-open==6.4.0
!pip install typer==0.9.4
!pip install pydantic==1.10.21
!pip install blis==0.7.11
!pip install spacy==3.5.4
!pip install coreferee
!python -m coreferee install en
!pip install pywsd
!python -m spacy download en_core_web_lg

# Run this cell once and Restart Session (Runtime -> Restart Session)

"""#Preparation
Some sentences to test the different algorithms that are showed in this notebook.
"""

import pprint

# These sentences and paragraph will be used in most of the snippets of code.
sample_sentence1 = "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on enabling machines "\
                  "to understand, interpret, and generate human language."
sample_sentence2 = "It involves various tasks such as sentiment analysis, machine translation, text summarization, and named entity recognition. "
sample_sentence3 = "Some notably successful Natural Language Processing systems developed in the 1960s were SHRDLU, a natural language system working "\
                   "in restricted 'blocks worlds' with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, "\
                   "written by J.Weizenbaum between 1964 and 1966."

paragraph = sample_sentence1 + " " + sample_sentence2 + " " + sample_sentence3

# The following sentence is included to test coreference solving
sample_sentence4 = "John's car broke down, so he called his mechanic and he said, 'It's not a big issue; I'll fix it quickly', " \
                   "however, John's wife, Sarah, was upset and told him: 'You're responsible for this mess! get it fixed before our dinner reservation.'"

# The following sentences are included to test disambiguation
sample_sentence5 = "I deposited all my savings in the bank"
sample_sentence6 = "I spent the morning sat near to the river's bank"

pp = pprint.PrettyPrinter(width=80, compact=True)

"""# 1 Tokenization

Basic Tokenization (split when whitespace is found).</br>
Imperfect </br>
Note:</br>
`- "psychoterapist,"`</br>
`- "'blocks"`</br>
`- "worlds'"`</br>
Punctuation signs are included in closest tokens.
"""

#Basic tokenization os sample_sentence3 using base python (split() on spaces)

words = sample_sentence3.split()
pp.pprint(words)

"""Or using regular expressions. Punctiation signs can be omitted."""

#Need to import re (regular expressions library)
import re

#split sample_sentence3 by groups of spaces, dot, comma or apostrophe.
words = re.split(r"[\s,'\.\"]+", sample_sentence3)
pp.pprint(words)

"""------------------------ NLTK

NLTK offers different tokenizers.</br>
**word_tokenize**: uses a kind of tokenization similar to the one used in the [Penn Treebank](https://paperswithcode.com/dataset/penn-treebank)</br>
Penn Treebank is an american english corpus with more than 4.5 millions of words extracted from news articles and documentation. It is one of the most known and used corpus for the evaluation of models for sequence labelling and uses a particular way or tokenizing that breaks apostrophes.
"""

#nltk base + nltk.tokenize package
import nltk
import nltk.tokenize as nltk_tokenize

#punkt_tab is an internal table in nltk to speedup tokenization
nltk.download('punkt_tab')

#Tokenize sample_sentence4 using standard word_tokenize
nltk_tokens = nltk_tokenize.word_tokenize(sample_sentence4)
pp.pprint(nltk_tokens)

"""


**word_punct_tokenize**:  uses the regular expression \w+|[^\w\s]+ to split the input. This tokenizer provides all words and all punctuation signs.

"""

#Tokenize sample_sentence4 using wordpunct tokenizer.
nltk_tokensp = nltk_tokenize.wordpunct_tokenize(sample_sentence4)
pp.pprint(nltk_tokensp)

#if you run the following code, you will get the same results.
#     regex_tokens = re.findall(r"\w+|[^\w\s]+", sample_sentence4)
#     pp.pprint(regex_tokens)

"""------------------------ spaCy

The spacy library tokenizes by default when a sentence becomes an nlp object.
"""

# spaCy is preloaded in Google Colab but we need an specific version to make coreference work.
# coreferee module is available in the large core english model
# (*) forget error messages related to dependencies.

# Dowload the large model to our disk

import spacy

# displacy is a module included in spacy to do do graphic representations of dependencies and named entities.
from spacy import displacy

# we load the model and its pipeline
nlp = spacy.load("en_core_web_lg")

# When we process a sentence "nlp(sentence)" spacy runs the tokenization and the complete pipeline.
ss3 = nlp(sample_sentence3)
ss4 = nlp(sample_sentence4)

# At this point we have all tokens of sample_sentence3 available and can display the results.
pp.pprint([token.text for token in ss3])
pp.pprint([token.text for token in ss4])

"""------------------------ gensim</br>
Gensim does only tokenize alphanumeric words that don't start with a number.
"""

#Install the gensim library

import gensim
#import required tokenize function
from gensim.utils import tokenize as gensim_tokenize

#The following is the
tokens = list(gensim_tokenize(sample_sentence3))
pp.pprint(tokens)

"""------------------------  Sentence Tokenization

Paragraphs can also be broken into sentences.


"""

#Split the paragraph into sentences
sentences = nltk_tokenize.sent_tokenize(paragraph)
pp.pprint(sentences)

"""# 2 Stop Words

list of stop words (all those words that do not contribute to the intrinsic meaning of the sentence.
"""

#Download and import the stop-word dictionary
nltk.download('stopwords')
from nltk.corpus import stopwords

#print the dictionary contents for english
pp.pprint(stopwords.words('english'))

"""Stop word removal with nltk"""

#load the previous stop-words to be used in a filter
stopw = set(stopwords.words('english'))

#Processing the complete sample paragraph. First WITH stopwords
original_tokens = nltk_tokenize.word_tokenize(paragraph)

#Then filtering the stop-words
filtered_tokens = [w for w in original_tokens if not w.lower() in stopw]

#print both results to compare
pp.pprint(original_tokens)
pp.pprint(filtered_tokens)

"""Stop-word removal with spaCy. spaCy pipeline includes stop-word processing and annotates them as ("is_stop") in order to be filtered if necessary"""

#run the pipeline with our complete paragraph
doc = nlp(paragraph)

# Filter those stopwords that have been flagged by the pipeline
filtered_words = [token.text for token in doc if not token.is_stop]

#print original paragraph and the final filtered token set
pp.pprint(paragraph)
pp.pprint(filtered_words)

"""Punctuation signs can also be removed if needed as spaCy flags all words as alpha or digit."""

# Filter punctuation signs
filtered_words = [token.text for token in doc if ((token.is_alpha or token.is_digit) and not token.is_stop)]

pp.pprint(doc)
pp.pprint(filtered_words)

"""#3 Stemming and Lemmatization

##Stemming (nltk)

Stemming consists of applying general rules to the text to remove morphological afixes (prefixes and sufixes like "un-xxxx", "dis-xxx" or "xxxx-ed", "xxxx-ing", "xxx-s").
In our case we will use PorterStemmer from nltk. It only removes suffixes.
"""

#Load the PorterStemmer
from nltk.stem import PorterStemmer
porter_stemmer = PorterStemmer()

#load some sample words
example_words = ["practice","practiced","practical", "practitioner", "undesirable"]

#and process (stemming) them.
for word in example_words:
   print (word, "->", porter_stemmer.stem(word))

"""##Lemmatization with nltk

Lemmatization is more sophisticated than stemming as it looks for the most basic form of a word that preserves the meaning.
For that reason it is not automatic and needs a dictionary.
"""

#Loading WordNet dictionary
nltk.download('wordnet')

#import required modules
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

#Some sample words (including their forms: noun, verb or adjective)
sample_words = [["cars","n"], ["programming", "v"], ["corpora", "n"], ["better", "a"],
                ["better", "n"], ["did", "v"], ["doing", "v"], ["does", "v"], ["longer", "a"]]

for word in sample_words:
  print(word[0], "->", lemmatizer.lemmatize(word[0], word[1]))

"""#4 Part of Speech / Part of Sentence

Both nltk and spaCy have resources to tag the words and word-groups as *Parts of Speech* or *Parts of a Sentence*.</br>
Parts of a Sentence depends on the Parts of Speech but also is compliant with a grammar. So both PoS must be run in the right order in the pipeline, and a grammar must be defined.

Parts of a Sentence (constituency)
```
S – Simple declarative sentence (e.g., I love NLP.)
SBAR – Subordinate clause (e.g., because I love NLP)
SBARQ – Direct question clause (e.g., What is NLP?)
SINV – Inverted declarative sentence (e.g., Rarely have I seen this.)
SQ – Inverted yes/no question (e.g., Is NLP useful?)
NP – Noun phrase (e.g., the big dog)
VP – Verb phrase (e.g., is running fast)
PP – Prepositional phrase (e.g., on the table)
ADJP – Adjective phrase (e.g., very happy)
ADVP – Adverb phrase (e.g., extremely quickly)
PRT – Particle (e.g., turn off, put up with)
CONJP – Conjunction phrase (e.g., not only ... but also)
FRAG – Fragmented sentence (e.g., Wow! or What a day!)
INTJ – Interjection (e.g., Oh no!, Wow!)
LST – List marker (e.g., 1., 2., A., B.)
NAC – Not a constituent (e.g., John, Jr., was there.)
NX – Used within certain noun phrases (e.g., tax-exempt bonds)
QP – Quantifier phrase (e.g., many of them, fewer than ten)
RRC – Reduced relative clause (e.g., the man seen in the park)
```

Part of Speech tags

```
NN – Singular noun (e.g., dog, house, car)
NNS – Plural noun (e.g., dogs, houses, cars)
NNP – Proper singular noun (e.g., John, London)
NNPS – Proper plural noun (e.g., Americans, Beatles)
PRP – Personal pronoun (e.g., I, you, he, she, it, we, they)
PRP$ – Possessive pronoun (e.g., my, your, his, her, its, our, their)
WP – Wh-pronoun (e.g., who, what, which, whom)
WP$ – Possessive wh-pronoun (e.g., whose)
VB – Base form (e.g., run, eat, go)
VBD – Past tense (e.g., ran, ate, went)
VBG – Gerund/present participle (e.g., running, eating, going)
VBN – Past participle (e.g., run, eaten, gone)
VBP – Present tense (except 3rd person singular) (e.g., run, eat, go)
VBZ – Present 3rd person singular (e.g., runs, eats, goes)
JJ – Adjective (e.g., big, happy, blue)
JJR – Comparative adjective (e.g., bigger, happier, bluer)
JJS – Superlative adjective (e.g., biggest, happiest, bluest)
RB – Adverb (e.g., quickly, softly, well)
RBR – Comparative adverb (e.g., faster, better, harder)
RBS – Superlative adverb (e.g., fastest, best, hardest)
DT – Determiner (e.g., the, a, an, this, that)
PDT – Predeterminer (e.g., all, both, half)
WDT – Wh-determiner (e.g., which, that, whatever)
CC – Coordinating conjunction (e.g., and, but, or, nor, so, yet)
IN – Preposition/subordinating conjunction (e.g., in, on, after, because)
CD – Cardinal number (e.g., one, two, 100)
EX – Existential "there" (e.g., There is a problem.)
MD – Modal verb (e.g., can, could, shall, should, will, would, must)
RP – Particle (e.g., up, off, over)
FW – Foreign word (e.g., bonjour, amigo)
LS – List item marker (e.g., 1, 2, A, B)
SYM – Symbol (e.g., $, %, +, =)
UH – Interjection (e.g., wow, oh, oops)
. – Sentence-final punctuation (e.g., period, exclamation mk, question mk)
, – Comma
: – Colon, semicolon
" – Quotation marks
( ) – Parentheses

```
"""

#svling is a library to display graphic (tree) representations of language structures
!pip install svgling
import svgling

# averageed_perceptron_tagger_eng is a pre-trained POS (Part-of-Speech) tagger
nltk.download('averaged_perceptron_tagger_eng')
from nltk import pos_tag, word_tokenize, RegexpParser

#print the original sentence as a reference:
pp.pprint(sample_sentence1)

#tag the words with their part-of-speech label
tagged = pos_tag(word_tokenize(sample_sentence1))
pp.pprint(tagged)

#define a grammar based on part-of-speech tags
grammar = RegexpParser("""
                    NP: {<DT>?<JJ>*<NN>} #To extract Noun Phrases
                    P: {<IN>}            #To extract Prepositions
                    V: {<V.*>}           #To extract Verbs
                    PP: {<P> <NP>}       #To extract Prepositional Phrases
                    VP: {<V> <NP|PP>*}   #To extract Verb Phrases
                    """)

#parse the previous tagged senence with the grammar and display it
output = grammar.parse(tagged)
output

"""Part-of-Speech</br>
In nltk, tokens are tagged with their part-of-speech tag
"""

#this is just to display the results in a readable table.
import pandas as pd

#tag the tokens
pos_tags = nltk.pos_tag(sample_sentence1.split())

#render as a table
pd.DataFrame(pos_tags).T

"""Part-of-Speech and Part-of-Sentence</br>
spaCy includes all the necessary stages and grammar in its pipeline to provide both groups of tagging.
"""

#read tags
spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in nlp(sample_sentence1)]
pd.DataFrame(spacy_pos_tagged).T

"""Dependency tags

```
nsubj – Nominal subject (e.g., She runs)
nsubjpass – Passive nominal subject (The book was written by him.)
dobj – Direct object (e.g., He eats apples)
iobj – Indirect object (e.g., She gave him a gift)
pobj – Object of a preposition (e.g., The kids playing on their computers)
pcomp – Complement of a preposition (e.g., I agree with what you said)
csubj – Clausal subject (e.g., What you said is true)
amod – Adjectival modifier (e.g., The red car)
appos – Apositional modifier (e.g., My friend John)
advmd – Adverbial modifier (e.g., She runs quickly.)
nmod – Noun modifier (e.g., the president of the company)
acl – Adjectival clause modifier (e.g., The person who called is here.)
det – Determiner (e.g. The book)
aux – Auxiliary verb (e.g., She is running)
auxpass – Passive auxiliary (e.g., He was seen)
cop – Copula verb (e.g., He is happy).
cc – Coordinating conjunction (e.g., and, or, but)
conj – Conjunct (e.g., I like apples and oranges.)
mark – Marker for subordinate clauses (e.g., because, although)
advcl – Adverbial clause modifier (e.g., She left because she was tired.)
neg – Negation modifier (e.g., He does not like it.)
expl – Expletive (e.g., There is a problem)
parataxis – Parataxis relation (e.g., Let’s go, he said.)
compound – Combinations of lexemes that behave as a word (e.g., apple juice)
prep – subtype of nmod (e.g., .)
attr – Noun phrase following a copula verb (e.g., this product is an ACME)
relcl – Relative clause that modifies a NP|NML (e.g., I bought the car that I wanted)
```

Dependency tags can also be calculated and graphically displayed using displacy.
"""

#use displacy to show the dependency tree
displacy.render(nlp(sample_sentence1), jupyter=True, options={'distance': 100, 'arrow_stroke': 1, 'arrow_width': 6})

"""# 5 Language Processing Pipeline

Spacy is designed to work as a pipeline.</br>
In fact, when we load a model like "en_core_web_xx", we are loading also a pipeline with several functions that are executed automatically when some text is loaded [nlp(text)].</br>
The following cleans the current english pipeline
"""

#clean current spaCy pipeline.
nlp = spacy.blank("en")

#show current spaCy pipeline stages (should be empty at this point)
print(nlp.pipe_names)

"""Event that the pipeline is empty we can tokenize any string (tokenization is always done). But we will have no more liguistic information as there's not any stage in the pipeline"""

#Process a sample sentence
tokens = nlp(sample_sentence3)

#show extracted information
empty_pipeline_table = [(token, token.tag_, token.pos_, token.lemma_) for token in tokens]
pd.DataFrame(empty_pipeline_table).T

"""now we will load a model (english, large) that includes a complete pipeline"""

nlp = spacy.load("en_core_web_lg")
print(nlp.pipe_names)

"""Let's tokenize and get detailed information"""

#process the same sample sentence with the new pipeline
tokens = nlp(sample_sentence3)

#Display some of the properties found by the different stages in the pipeline
complete_pipeline_table = [(token, token.pos_, token.tag_, token.lemma_) for token in tokens]
pd.DataFrame(complete_pipeline_table).T

"""# 6 Named Entities

Our spaCy pipeline includes the resolution of Named Entities ('ner')
"""

#Named entities are found during the "ner" stage of the pipeline
entities = [(ent.text, ent.label_, spacy.explain(ent.label_)) for ent in tokens.ents]

#show them as a table
pd.DataFrame(entities).T

"""displacy (included in spaCy offers a more friendly representation"""

displacy.render(tokens, style="ent", jupyter=True)

"""The 2 following cells need to complete a registration in Huggingface to provide a valid token (api_key).</br>
Huggingface is actually the most important repository of opensource AI models.</br>
If you are interested, just check in in https://huggingface.co/join and get a token in https://huggingface.co/settings/tokens [Create new token].
Without a token you won't be able to download the used models, so the following two cells will fail.</br>
</br>
The following code demonstrates how to detect Named Entities with a trained AI model. First, we will use this model: https://huggingface.co/dslim/distilbert-NER

If you are not interested, just skip the two next cells. (I have left my output).


"""

from huggingface_hub import InferenceClient

client = InferenceClient(provider="hf-inference", api_key="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")

result = client.token_classification(text=sample_sentence3, model="dslim/distilbert-NER")
for token in result:
  print(token.word, token.entity_group, token.score)

"""This is my output from the previous cell:
```
Natural Language Processing MISC 0.891736626625061
SH MISC 0.5617724657058716
##RD ORG 0.5021995902061462
##L ORG 0.5555049777030945
##U ORG 0.6675312519073486
E ORG 0.4286145269870758
##L ORG 0.6228836178779602
##I ORG 0.5453646183013916
##Z ORG 0.6251108646392822
##A ORG 0.61518394947052
Roger MISC 0.9791512489318848
##ian MISC 0.9472392797470093
J PER 0.9976842403411865
. Weizenbaum PER 0.9985617995262146
```

"""

result = client.token_classification(text=sample_sentence3, model="dslim/bert-large-NER")
for token in result:
  print(token.word, token.entity_group, token.score)

"""This is my output from the previous cell:

```
Natural Language Processing MISC 0.8586603999137878
SHRDL ORG 0.5619422197341919
##U MISC 0.40039971470832825
E ORG 0.5633866786956787
##L MISC 0.3449726402759552
##IZ ORG 0.3782796859741211
##A MISC 0.4661734700202942
Rogerian MISC 0.9579911828041077
J. Weizenbaum PER 0.9293615221977234
```

# 7 Coreference Resolution

Coreference resolution's stage is not included by default in the spaCy pipeline. So it must be loaded and attached
"""

#Add coreference resolution to the pipeline
import coreferee
nlp.add_pipe("coreferee")

#Show the complete pipeline to check if coreferee has been included
print(nlp.pipe_names)

#Process a sample sentence and print it
doc4 = nlp(sample_sentence4)
pp.pprint(sample_sentence4)

# Display coreference clusters and resolution
print("\n Coreference clusters:")
for cluster in doc4._.coref_chains:
    print(f"\n{cluster.pretty_representation}")
    for mention in cluster:
        span = doc4[mention.root_index]
        print(f"  - '{span.text}' (Token index: {span.i})")

"""# 8 Disambiguation

Pywsd is a library that implements different Word Sense Disambiguation (WSD) technics. One of them is Lesk Algorithm.
"""

#install pyWSD library
from pywsd.lesk import simple_lesk

#define an ambiguous word
ambiguous_word = "bank"

#Resolve the ambiguity for two different sentences
for sentence in [sample_sentence5, sample_sentence6]:
  answer = simple_lesk(sentence, ambiguous_word)
  print(f"\n\nSentence: {sentence}")
  print(f"Resolved sense: {answer.name()} - {answer.definition()}")

"""#Activity

With the following text:

> Sarah, a project manager at GreenTech Innovations , hurried to the bank  on Monday to finalize a loan for her company’s new solar farm. “They’re  counting on me,” she muttered, clutching her father’s  old briefcase. The bank’s  manager, Mr. Patel, greeted her warmly, but he  seemed distracted by a report about rising interest rates. “It’s  a critical time for GreenTech ,” Sarah insisted, hoping to secure the funds.   
After the meeting, she  drove to Willow Creek , a quiet spot where she’d  often gone hiking with her  brother. As she  walked along the river’s  edge, she  noticed a bank  eroded by recent storms. “This bank  needs reinforcement,” she  thought, recalling GreenTech’s  environmental projects.

Do the following in separated cells:
1. Split the paragraph in sentences using nltk (see section 1)
2. Tokenize first sentence using the Penn Treebank style (section 1)
3. Get stemmings for all words of the first sentence using PorterStemmer in ntlk(section 3)
4. Find named entities in ALL paragraph and show them using displacy (section 6)
5. Solve coreferences in first sentence (section 7)
"""

act_paragraph = "Sarah, a project manager at GreenTech Innovations, hurried to the bank on Monday to finalize a loan for her company’s new solar farm."\
            "'They’re counting on me,' she muttered, clutching her father’s old briefcase. "\
            "The bank’s manager, Mr. Patel, greeted her warmly, but he seemed distracted by a report about rising interest rates. "\
            "'It’s a critical time for GreenTech,' Sarah insisted, hoping to secure the funds. "\
            "After the meeting, she drove to Willow Creek, a quiet spot where she’d often gone hiking with her brother. "\
            "As she walked along the river’s edge, she noticed a bank eroded by recent storms. "\
            "'This bank needs reinforcement,' she thought, recalling GreenTech’s  environmental projects."

act_first_sentence = "Sarah, a project manager at GreenTech Innovations, hurried to the bank on Monday to finalize a loan for her company’s new solar farm."

"""1. Split **the paragraph** into sentences"""

# Here I Download the necessary tokenizer data and import necessary libraries
import nltk
nltk.download('punkt')
def split_into_sentences(act_paragraph):
    sentences = nltk.sent_tokenize(act_paragraph)
    return sentences

#Here i put the full paragraph :
act_paragraph = "Sarah, a project manager at GreenTech Innovations, hurried to the bank on Monday to finalize a loan for her company’s new solar farm."\
            "'They’re counting on me,' she muttered, clutching her father’s old briefcase. "\
            "The bank’s manager, Mr. Patel, greeted her warmly, but he seemed distracted by a report about rising interest rates. "\
            "'It’s a critical time for GreenTech,' Sarah insisted, hoping to secure the funds. "\
            "After the meeting, she drove to Willow Creek, a quiet spot where she’d often gone hiking with her brother. "\
            "As she walked along the river’s edge, she noticed a bank eroded by recent storms. "\
            "'This bank needs reinforcement,' she thought, recalling GreenTech’s  environmental projects."
sentences = split_into_sentences(act_paragraph)

for i, sentence in enumerate(sentences, 1):
    print(f"{i}. {sentence.strip()}")

#Basic tokenization os sample_sentence3 using base python (split() on spaces)

words = act_paragraph.split()
pp.pprint(words)

"""2. Tokenize **first sentence** using the Penn Treebank style of tokenization"""

#Tokenize sample_sentence4 using standard word_tokenize
nltk_tokens = nltk_tokenize.word_tokenize(act_first_sentence)
pp.pprint(nltk_tokens)

"""3. Get stemmings for all words of the **first sentence** using PorterStemmer in ntlk

preguntar cual es el resultado ideal y explicar porque se come inventa palabras
"""

# Importar librerías necesarias
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import PorterStemmer

# Descargar datos necesarios
nltk.download('punkt')

def stem_first_sentence(paragraph):
    # Obtener la primera oración
    sentences = sent_tokenize(paragraph)
    if not sentences:
        return []
    first_sentence = sentences[0]

    # Tokenizar la primera oración
    tokens = word_tokenize(first_sentence)

    # Inicializar Porter Stemmer
    stemmer = PorterStemmer()

    # Aplicar stemming a cada palabra
    stems = [(token, stemmer.stem(token)) for token in tokens]

    return stems

# Texto de ejemplo
paragraph = "Sarah, a project manager at GreenTech Innovations, hurried to the bank on Monday to finalize a loan for her company’s new solar farm."

# Llamar la función y obtener los stemmings
stems = stem_first_sentence(paragraph)

# Imprimir resultados
print("Original Word → Stem")
for word, stem in stems:
    print(f"{word} → {stem}")

"""Yes, the result is ideal for stemming because PorterStemmer does not aim to produce meaningful words, only to reduce them to their root form. This is useful for information retrieval, text processing, and reducing word variations in NLP tasks.

However, if you need a more advanced approach, you can try:

LancasterStemmer (more aggressive but sometimes over-stems words).

Lemmatization (WordNetLemmatizer) (more precise and keeps real words).

4. Find named entities in **ALL paragraph** and show them using displacy
"""

# When we process a sentence "nlp(sentence)" spacy runs the tokenization and the complete pipeline.
ss3 = nlp(act_paragraph)
ss4 = nlp(act_paragraph)

# At this point we have all tokens of sample_sentence3 available and can display the results.
pp.pprint([token.text for token in ss3])
pp.pprint([token.text for token in ss4])

#use displacy to show the dependency tree
displacy.render(nlp(act_paragraph), jupyter=True, options={'distance': 100, 'arrow_stroke': 1, 'arrow_width': 6})

"""Part of Speech (POS) (e.g., NOUN, VERB, ADP, etc.).

Dependency label (e.g., nsubj, pobj, prep).

Arrows that indicate grammatical relationships

Helps in understanding sentence structure for NLP tasks (e.g., text parsing, named entity recognition).

Useful in chatbots, grammar correction, and information extraction.

Can aid in syntactic analysis for machine learning models.
"""

# Process the text with spaCy
doc = nlp(act_paragraph)

# Print named entities
print("Named Entities:")
for ent in doc.ents:
    print(f"{ent.text} ({ent.label_})")  # Text + Entity type (e.g., PERSON, ORG, DATE)

# Visualize with displacy (HTML rendering in Jupyter/Colab)
displacy.render(doc, style="ent", jupyter=True)

"""5. Solve the coreferences present in the **first sentence**"""

#Process a sample sentence and print it
doc4 = nlp(act_first_sentence)
pp.pprint(act_first_sentence)

# Display coreference clusters and resolution
print("\n Coreference clusters:")
for cluster in doc4._.coref_chains:
    print(f"\n{cluster.pretty_representation}")
    for mention in cluster:
        span = doc4[mention.root_index]
        print(f"  - '{span.text}' (Token index: {span.i})")

