# -*- coding: utf-8 -*-
"""Big_Data_Parallel_Programming.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ll8dYENyx8X2u-5fRMq3fWvjn14kyzpS

![descarga.jpeg](data:image/jpeg;base64,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)

### Hotel Booking Project
***Martina Sagastume***

***Hugo Brines***

# Creating and filling our dataframe

This process ensures the dataset is cleaned systematically, addressing common issues like missing values and incorrect data types. Handling missing values appropriately depending on the context (numeric vs. categorical) is crucial for maintaining data integrity and usefulness in analyses or machine learning models.

## The initial database
"""

# pip install pandas

import os
import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

def initiate_data() :
  df = pd.read_csv("hotel_bookings.csv")
  return df
df = initiate_data()

"""We need to get an idea of the data"""

# Display the first few rows of the DataFrame
print(df.head())

# Get a summary of missing values in each column
print(df.isnull().sum())

## Find the missing value, show the total null values for each column and sort it in descending order
df.isnull().sum().sort_values(ascending=False)[:10]

# Select the 'country' column
country = df['country']
print(country.describe())

"""We can now say that there are 118 902 (the number of non-null values in country) + 488 (the number of null values in country) = 119 390 rows in our dataset !"""

df.head(10)

"""## Missing values







































"""

## If no id of agent or company is null, just replace it with 0
df[['agent','company']] = df[['agent','company']].fillna(0.0)

## For the missing values in the country column, replace it with mode (value that appears most often)
df['country'].fillna(df.country.mode().to_string(), inplace=True)


## for missing children value, replace it with rounded mean value
df['children'].fillna(round(df.children.mean()), inplace=True)

## Drop Rows where there is no adult, baby and child
df = df.drop(df[(df.adults+df.babies+df.children)==0].index)

# Verify that there are no more missing values
print(df.isnull().sum())

# remove duplicate rows from the DataFrame, this is an important step in data cleaning to ensure that each row in the dataset is unique
df = df.drop_duplicates()

df.info()

"""
By taking a view of the information of this dataset, we can see is a large dataset, with many variables. We have a total of 11 float which means numbers, we have 9 integers, and variables with object answer (words) are a total of 11. In total we have 30 different variables and furthermore we can state a target variable to see our main goal."""

## convert datatype of these columns from float to integer
df[['children', 'company', 'agent', 'hotel']] = df[['children', 'company', 'agent', 'hotel']].astype('int64')

df.head(5)

df.info()

## If no id of agent or company is null, just replace it with 0
df[['agent','company']] = df[['agent','company']].fillna(0.0)

df.info()

"""# Some questions about the dataset

**WHICH MONTH IS THE BUSIEST**
"""

reservations_per_month = df['arrival_date_month'].value_counts()
reservations_per_month_sorted = reservations_per_month.sort_values(ascending=False)
print("Number of reservations every month :")
print(reservations_per_month_sorted)

# Plot the data
reservations_per_month_sorted.plot(kind='bar', color='skyblue')
plt.title('Number of Reservations per Month')
plt.xlabel('Month')
plt.ylabel('Number of Reservations')
plt.xticks(rotation=45)  # Rotate the month labels for better readability
plt.show()

# Check the maximum year in the dataset to determine the last year
df['arrival_date_year'] = pd.to_numeric(df['arrival_date_year'])  # Ensure it's numeric
last_year = df['arrival_date_year'].max()
print("The last year in the dataset is:", last_year)

# Filter the data for the last year
last_year_data = df[df['arrival_date_year'] == last_year]
last_year_data = df[df['arrival_date_year'] == last_year]

# Count the number of reservations for each month of the last year
reservations_per_month_last_year = last_year_data['arrival_date_month'].value_counts()

# Sort the counts by the order of months if they are out of order
month_order = ["January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"]
reservations_per_month_last_year = reservations_per_month_last_year.reindex(month_order)

# Print the sorted values
print("Number of reservations every month during the last year:")
print(reservations_per_month_last_year)

# Now we group by hotel
reservations_by_hotel_and_month = df.groupby(['hotel', 'arrival_date_month']).size()
reservations_by_hotel_and_month_sorted = reservations_by_hotel_and_month.sort_values(ascending=False)
print("Number of reservations each month for each hotel :")
print(reservations_by_hotel_and_month_sorted)

"""**LOYALTY OF CLIENTS**"""

# Are clients loyal to the hotels ?

loyal_guests_count = df[df['is_repeated_guest'] == 1]['is_repeated_guest'].count() # Total amount of loyal clients
total_guests_count = df['is_repeated_guest'].count() # Total amount of clients
loyal_guests_percentage = (loyal_guests_count / total_guests_count) * 100

print("There are " + str(loyal_guests_percentage) + " % loyal clients")

# Now we group by hotel
loyalty_by_hotel = df.groupby('hotel')['is_repeated_guest'].mean() * 100
print("Loyalty percent for each hotel :")
print(loyalty_by_hotel)

"""We can see that the Resort Hotel has more loyal clients rather than City Hotel

**NUMBER OF CLIENTS EVERY YEAR**
"""

clients_per_year = df.groupby('arrival_date_year').size()
print("Number of clients every year :")
print(clients_per_year)

months = pd.date_range(start='2015-07-01', end='2017-08-31', freq='M')
months_str = months.strftime('%B %Y')
clients_per_month = pd.DataFrame(index=months_str, columns=['Number of Clients'])
clients_per_month['Number of Clients'] = 0

# Count the total amount of clients every month
for year in range(2015, 2018):
    for month in range(1, 13):
        month_name = pd.Timestamp(year, month, 1).strftime('%B %Y')
        if month_name in clients_per_month.index:
            clients_per_month.loc[month_name, 'Number of Clients'] = len(df[(df['arrival_date_year'] == year) & (df['arrival_date_month'] == month_name.split()[0])])

fig, ax = plt.subplots(figsize=(12, 6))
ax.bar(clients_per_month.index, clients_per_month['Number of Clients'])
ax.set_xlabel('Month')
ax.set_ylabel('Number of clients')
ax.set_title('Number of clients every month')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

months = pd.date_range(start='2015-07-01', end='2017-08-31', freq='M')

months_str = months.strftime('%B %Y')

hotels = df['hotel'].unique()
clients_per_month_by_hotel = {hotel: pd.DataFrame(index=months_str, columns=['Number of Clients']) for hotel in hotels}

for hotel, df_hotel in df.groupby('hotel'):
    clients_per_month_by_hotel[hotel]['Number of Clients'] = 0

for hotel, df_hotel in df.groupby('hotel'):
    for year in range(2015, 2018):
        for month in range(1, 13):
            month_name = pd.Timestamp(year, month, 1).strftime('%B %Y')
            if month_name in clients_per_month_by_hotel[hotel].index:
                clients_per_month_by_hotel[hotel].loc[month_name, 'Number of Clients'] = len(df_hotel[(df_hotel['arrival_date_year'] == year) & (df_hotel['arrival_date_month'] == month_name.split()[0])])

fig, axes = plt.subplots(nrows=len(hotels), ncols=1, figsize=(12, 6*len(hotels)), sharex=True)
for i, (hotel, df_hotel) in enumerate(df.groupby('hotel')):
    ax = axes[i]
    ax.bar(clients_per_month_by_hotel[hotel].index, clients_per_month_by_hotel[hotel]['Number of Clients'])
    ax.set_ylabel('Number of clients')
    ax.set_title(f'Number of clients every month - {hotel}')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""## How many booking were cancelled?"""

import seaborn as sns
import matplotlib.pyplot as plt

def plot(x, y, x_label='Booking Cancelled (No = 0, Yes = 1)', y_label='Booking (%)', title=None, figsize=(10, 6), type='bar'):
    # Create a new figure with specified figure size
    plt.figure(figsize=figsize)
    ax = plt.gca()  # Get the current Axes instance on the current figure

    # Determine the type of plot
    if type == 'bar':
        sns.barplot(x=x, y=y, ax=ax)  # Correctly passing x and y as keyword arguments
    elif type == 'line':
        sns.lineplot(x=x, y=y, ax=ax, sort=False)  # Assuming sort=False is intended for lineplot

    # Set labels and title
    ax.set_xlabel(x_label)
    ax.set_ylabel(y_label)

    if title:
        ax.set_title(title)

    # Display the plot
    plt.show()

# Example usage
x = ['No', 'Yes']  # Example x data
y = [80, 20]       # Example y percentages
plot(x, y)  # Calling the plot function

"""Conversely, the 20% for "Yes" indicates a significantly smaller portion of the dataset where the outcome was 'Yes'. In the context of hotel bookings, this means that only 20% of the bookings were cancelled.

An 80% non-cancellation rate is generally a positive indicator of customer retention and could imply customer satisfaction or other factors that encourage customers to follow through with their bookings.

For the hotel management, a 20% cancellation rate might prompt investigation into reasons for cancellations to identify any common issues or trends, such as pricing, service quality, or external factors like travel disruptions.

# Parallelization (Martina):
As we understand the topic parallelized, are tasks that can take action at the same time without altering each other but affecting the final result. Each tasks could have a different time until it ends the processing, but would not alter the other time. Using this method would help us to investigate good the dataset and find out the final questions that we want to adress by performing multiple tasks.
"""

!pip install pyspark

from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").appName("Hotel Bookings Analysis").getOrCreate()

# Load the data
df = spark.read.csv('hotel_bookings.csv', header=True, inferSchema=True)
df.printSchema()  # Check the schema to understand data types

# Assuming 'df' is your DataFrame
null_counts = df.filter(df['arrival_date_month'].isNull()).count()
print(f"Number of nulls in arrival_date_month: {null_counts}")

from pyspark.sql.functions import month, avg, col

df = df.withColumn("arrival_date_month", month(col("arrival_date_month")))
cancellation_by_month = df.groupBy("arrival_date_month").agg(avg("is_canceled").alias("cancel_rate"))
cancellation_by_month.show()

cancellation_by_deposit = df.groupBy("deposit_type").agg(avg("is_canceled").alias("cancel_rate"))
cancellation_by_deposit.show()

"""## Paralize training and testing"""

!pip install pyspark

"""## Speed benchmark with spark"""

from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Hotel Bookings Analysis").getOrCreate()
df_spark = spark.read.csv("hotel_bookings.csv", header=True, inferSchema=True)

from google.colab import drive
drive.mount('/content/drive')

#Starting a spark session
from pyspark.sql import SparkSession

try:
    spark = SparkSession \
        .builder \
        .appName("Spark Lab2") \
        .config("spark.some.config.option", "some-value") \
        .getOrCreate()
except:
    Pass

spark

spark.sparkContext

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# 
# # loading csv file with Spark
# housing = spark.read.csv("hotel_bookings.csv", header="true")

"""# with python"""

import pandas as pd

df_pandas = pd.read_csv("hotel_bookings.csv")

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# 
# # loading csv file with Pandas
# df_pandas = pd.read_csv("hotel_bookings.csv")

"""As we cn observed and compare the time running between spark and python we see that in this scenario python runs faster. it depends when you run it.

# Another test
"""

# π calculation code

import random

num_samples = 1000 # you can change this number, e.g. try 1000000

def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y < 1

def spark_pi_calc():
    # here we do the pi calcaulation using Spark
    count = spark.sparkContext.parallelize(range(0, num_samples)).filter(inside).count()
    return (4.0 * count / num_samples)

def python_pi_calc():
    # here we do the same calculation with python list comprehension
    count = sum([inside(_) for throw in range(num_samples)])
    return (4.0 * count / num_samples)

print("[Spark] Pi is roughly:", spark_pi_calc())

print("[Python] Pi is roughly:", python_pi_calc())

import timeit, time

max_time = 10 # you can also try 1, 2, 5, and 10 depending on your hardware performance.

print('Running experiment. This may take a few minutes to run.')
print('You can change max_time value to increase or decrease run time.')
print('(please wait)')

num_samples = 1000
steps = []
python_times = []
sparks_times = []

def my_timeit(func):
    runs = 3  # If the experiment is still taking too much time to run, you may decrease this value as well.
    dtime = timeit.timeit(func, number=runs)
    elapsed = dtime/runs
    return elapsed

start = time.time()
while True:
    pt = my_timeit(python_pi_calc)
    st = my_timeit(spark_pi_calc)
    python_times.append(pt)
    sparks_times.append(st)
    steps.append(num_samples)
    print(min(int(pt * 100), max_time*100), '/', max_time*100)
    if pt > max_time:
        break

    if pt > max_time:
        break
    elif pt < 0.1:
        num_samples = num_samples * 10
    else:
        num_samples = num_samples * 2
print(f"Done! Total time = {time.time()-start:.2f}s")

"""if we change the number of samples, therefore the tota time changes also. this is due to the number of nodes that the session would create ad alocate work."""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

plt.figure()
plt.plot(steps, python_times, color='red', label='python')
plt.plot(steps, sparks_times, color='blue', label='spark')
plt.legend()
plt.xlabel('number of samples', fontsize=12)
plt.ylabel('running time (seconds)', fontsize=12)
plt.title('Speed benchmark (π calculation)')
plt.show()

##Gradient Boosting

from pyspark.ml.regression import GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator

gb = GBTRegressor(featuresCol="features", labelCol="label")

# Fit training data
gbModel = gb.fit(trainSet)

predictions = gbModel.transform(testSet)
predictions.select("prediction", "label", "features").show(5)

from pyspark.ml.evaluation import RegressionEvaluator

evaluator = RegressionEvaluator(
    labelCol="label",
    predictionCol="prediction",
    metricName="rmse"
)

rmse = evaluator.evaluate(predictions)
print("Root Mean Squared Error (RMSE) on test data = " + str(rmse))

"""# RDD

## Correlation matrix
"""

from pyspark import SparkContext
sc = SparkContext.getOrCreate()

# Load data into an RDD from a CSV file
file_path = 'hotel_bookings.csv'
rdd = sc.textFile(file_path)

# Parse: Since RDDs do not understand the schema of a CSV, you need to parse the RDD to split each line into its respective fields:
def parse_line(line):
    return line.split(',')

parsed_rdd = rdd.map(parse_line)

# Remove the header first if necessary
header = parsed_rdd.first()  # remove the header
parsed_rdd = parsed_rdd.filter(lambda row: row != header)

# Map transformation to pair each row with key as month and value as cancellation status
month_rdd = parsed_rdd.map(lambda x: (x[2], (int(x[1]), 1)))

# Reduce by key to sum up cancellations and count rows per month
reduced_rdd = month_rdd.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))

# Compute the average cancellation rate per month
final_rdd = reduced_rdd.mapValues(lambda x: x[0] / x[1])

results = final_rdd.collect()
for result in results:
    print(result)

"""Feature Identifier: This is the first element of each tuple (e.g., '342', '737', '14'). These identifiers likely represent either agent IDs, hotel IDs, or some other categorical variable in your dataset.

Correlation Value: This is the second element of each tuple, which is a floating-point number representing the correlation coefficient between the feature and the cancellation outcome. The correlation value ranges between -1 and 1:

Positive Values closer to 1 indicate a strong positive correlation, meaning as the feature value increases, the likelihood of cancellation also increases.
Negative Values closer to -1 indicate a strong negative correlation, suggesting that as the feature value increases, the likelihood of cancellation decreases.
Values Close to 0 suggest little to no linear correlation between the feature and the cancellation.
"""

import pandas as pd

# Creating a placeholder for the correlation values
correlation_data = {col: [] for col in feature_columns}
for col_i in feature_columns:
    for col_j in feature_columns:
        corr = df.stat.corr(col_i, col_j)
        correlation_data[col_i].append(corr)

# Convert dictionary to Pandas DataFrame
correlation_matrix = pd.DataFrame(correlation_data, index=feature_columns)

## Correlation Among attributes
from pyspark.sql import SparkSession
from pyspark.ml.stat import Correlation
from pyspark.ml.feature import VectorAssembler

df.head()

# Assemble the attributes into a vector
assembler = VectorAssembler(
    inputCols=["lead_time", "total_of_special_requests", "previous_cancellations", "adr"],
    outputCol="features")
hotelAttrs = assembler.transform(df)

# Compute the correlation matrix
correlation_matrix = Correlation.corr(hotelAttrs, "features").head()

# Extract the correlation matrix from the result
corr_matrix = correlation_matrix[0].toArray()

# Print the correlation matrix
print(corr_matrix)

"""## Model Training to Predict Cancellations

"""

#Random Forest
from pyspark.ml.regression import RandomForestRegressor

# Split the data into training and test sets (80% training, 20% test)
trainSet, testSet = df.randomSplit([0.8, 0.2], seed=42)

# Define the columns you want to use as features
feature_columns = ['lead_time', 'total_of_special_requests', 'previous_cancellations', 'adr']  # Add or remove columns as needed

# Create a VectorAssembler to combine feature columns into a single vector column
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")

# Transform the data to have the features column
trainSet = assembler.transform(trainSet)
testSet = assembler.transform(testSet)

# Initialize the RandomForestRegressor
rf = RandomForestRegressor(featuresCol="features", labelCol="is_canceled")

# Fit the model to the training data
rfModel = rf.fit(trainSet)

from pyspark.ml.evaluation import RegressionEvaluator

# Make predictions on the test data
predictions = rfModel.transform(testSet)
predictions.select("prediction", "is_canceled", "features").show(5)

# Create a RegressionEvaluator object for RMSE
evaluator = RegressionEvaluator(labelCol="is_canceled", predictionCol="prediction", metricName="rmse")

# Calculate the RMSE on the test data
rmse = evaluator.evaluate(predictions)
print("Root Mean Squared Error (RMSE) on test data = " + str(rmse))

"""this means that the prediction of the cancelation rate would decrease"""

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import LogisticRegression

assembler = VectorAssembler(inputCols=["lead_time", "total_of_special_requests"], outputCol="features")
df = assembler.transform(df)

lr = LogisticRegression(featuresCol="features", labelCol="is_canceled")
lrModel = lr.fit(df)

# Example: Print model coefficients
print("Coefficients:", str(lrModel.coefficients))

"""These are some examples of what we can perform to use paralellization

*   Exploratory Data Analysis (EDA): Certain EDA tasks, such as calculating summary statistics, visualizing data distributions, or computing correlations between variables, can be parallelized.
*   When training machine learning models, you can parallelize the training process by training multiple models simultaneously on different subsets of your data or using techniques like mini-batch training. Many machine learning libraries, such as scikit-learn or TensorFlow, support parallelization
* When performing cross-validation to evaluate model performance, parallelize the cross-validation (random forest) process by running multiple folds in parallel. This can speed up the evaluation of multiple models and parameter combinations.

## Big Data Parallelization Strategy Plan:
"""

# Check unique values in the hotel_type column
print(df['hotel'].unique())

# Filter df_clean to only show rows where the hotel type is 'Resort Hotel'
resort_hotel_df = df[df['hotel'] == 'Resort Hotel']
city_hotel_df = df[df['hotel'] == 'City Hotel']

print(resort_hotel_df.head())

print(city_hotel_df.head())

# Display summary statistics for the 'Resort Hotel' data
print(resort_hotel_df.describe())

print(city_hotel_df.describe())

# Calculate the cancellation rate for 'Resort Hotel'
cancellation_rate = resort_hotel_df['is_canceled'].mean()
print(f"Cancellation Rate for 'Resort Hotel': {cancellation_rate:.2f}")

cancellation_rate2 = city_hotel_df['is_canceled'].mean()
print(f"Cancellation Rate for 'City Hotel': {cancellation_rate:.2f}")

# Print the DataFrame's column names and data types
print(df.dtypes)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'is_canceled' is the column you're predicting
X = df.drop('is_canceled')
y = df['is_canceled']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Assuming df_clean is preprocessed with 'is_canceled' as the target variable
X = df.drop('is_canceled', axis=1)
y = df['is_canceled']

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Fit RandomForest
forest = RandomForestClassifier(n_estimators=100, random_state=42)
forest.fit(X_train, y_train)

# Get feature importances
importances = forest.feature_importances_

"""HUGO


"""

# First, let's copy the dataframe

df_subset = df.copy()

## Make the new column which contain 1 if guest received the same room which was reserved otherwise 0
df_subset['room'] = 0
df_subset.loc[ df_subset['reserved_room_type'] == df_subset['assigned_room_type'] , 'room'] = 1
df_subset = df_subset.drop(['assigned_room_type','reserved_room_type'],axis=1)

## Make the new column which contain 1 if the guest has cancelled more booking in the past
## than the number of booking he did not cancel, otherwise 0

df_subset['net_cancelled'] = 0
df_subset.loc[ df_subset['previous_cancellations'] > df_subset['previous_bookings_not_canceled'] , 'net_cancelled'] = 1
df_subset = df_subset.drop(['previous_cancellations','previous_bookings_not_canceled'],axis=1)

## Remove reservation_status column
## because it tells us if booking was cancelled
df_subset = df_subset.drop(['reservation_status'], axis=1)

df_subset.info()

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# List of categorical columns
categorical_columns = ['hotel', 'arrival_date_month', 'meal', 'market_segment',
                       'distribution_channel', 'deposit_type', 'customer_type']

# Apply LabelEncoder to each categorical column
for column in categorical_columns:
    print(column, df_subset[column].dtype)
    df_subset[column] = label_encoder.fit_transform(df_subset[column])

# Check the result
df_subset.info()

# Replace integer 0 with string 'UNKNOWN'
df_subset['country'] = df_subset['country'].replace(0, 'UNKNOWN')

# Now, we can proceed with label encoding
label_encoder = LabelEncoder()

# Apply LabelEncoder to the 'country' column
df_subset['country'] = label_encoder.fit_transform(df_subset['country'])

# Check the result
print(df_subset['country'].unique())

df_subset.info()

# Convert 'reservation_status_date' to datetime
df_subset['reservation_status_date'] = pd.to_datetime(df_subset['reservation_status_date'])
df_subset = df_subset.drop(['reservation_status_date'], axis=1)
# Now you can proceed with your machine learning model

# Let's split the data into train and test (75% train, 25% test)

def data_split(df, label):

    from sklearn.model_selection import train_test_split

    X = df.drop(label, axis=1)
    Y = df[label]

    x_train, x_test, y_train, y_test = train_test_split(X,Y,random_state=0)

    return x_train, x_test, y_train, y_test



x_train, x_test, y_train, y_test = data_split(df_subset, 'is_canceled')

print("len(x_train) = ", len(x_train))
print("len(y_train) = ", len(y_train))
print("len(x_test) = ", len(x_test))
print("len(y_test) = ", len(y_test))

print("\nproportion of train = ", len(x_train) / (len(x_train) + len(x_test)))

def train(x_train, y_train):
    from sklearn.tree import DecisionTreeClassifier

    clf = DecisionTreeClassifier(random_state=0)
    clf.fit(x_train,y_train)

    return clf


clf = train(x_train, y_train)

def Score(clf,x_train,y_train,x_test,y_test):
    train_score = clf.score(x_train,y_train)
    test_score = clf.score(x_test,y_test)

    print("========================================")
    print(f'Training Accuracy of our model is: {train_score}')
    print(f'Test Accuracy of our model is: {test_score}')
    print("========================================")


Score(clf,x_train,y_train,x_train,y_train)